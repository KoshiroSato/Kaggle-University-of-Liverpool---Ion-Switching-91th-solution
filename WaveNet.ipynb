{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "## https://www.kaggle.com/cdeotte/one-feature-model-0-930                 ##\n",
    "## https://www.kaggle.com/siavrez/wavenet-keras                           ##\n",
    "## https://www.kaggle.com/sggpls/wavenet-with-shifted-rfc-proba           ##\n",
    "## https://www.kaggle.com/nxrprime/wavenet-with-shifted-rfc-proba-and-cbr ##\n",
    "## I learned a lot from their notebooks. Thank you.                       ##\n",
    "############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jx8dkVKCb7e6"
   },
   "outputs": [],
   "source": [
    "import os, random, time, gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow.keras import models, losses\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.utils import get_custom_objects\n",
    "from tensorflow.keras.callbacks import Callback, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LMw2mUDhb7fA"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 200\n",
    "NNBATCHSIZE = 16\n",
    "GROUP_BATCH_SIZE = 4000\n",
    "SEED = 2020\n",
    "SPLITS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    train = pd.read_csv('../input/data-without-drift/train_clean.csv', \n",
    "                        dtype={'time': np.float32, 'signal': np.float32, 'open_channels':np.int32})\n",
    "    \n",
    "    test  = pd.read_csv('../input/data-without-drift/test_clean.csv', \n",
    "                        dtype={'time': np.float32, 'signal': np.float32})\n",
    "    \n",
    "    sub  = pd.read_csv('../input/liverpool-ion-switching/sample_submission.csv', \n",
    "                       dtype={'time': np.float32})\n",
    "    \n",
    "    Y_train_proba = np.load('../input/ion-shifted-rfc-proba/Y_train_proba.npy')\n",
    "    Y_test_proba = np.load('../input/ion-shifted-rfc-proba/Y_test_proba.npy')\n",
    "    \n",
    "    for i in range(11):\n",
    "        train[f\"proba_{i}\"] = Y_train_proba[:, i]\n",
    "        test[f\"proba_{i}\"] = Y_test_proba[:, i]\n",
    "\n",
    "    return train, test, sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batching(df, batch_size):\n",
    "    df['group'] = df.groupby(df.index//batch_size, sort=False)['signal'].agg(['ngroup']).values\n",
    "    df['group'] = df['group'].astype(np.uint16)\n",
    "    return df\n",
    "\n",
    "\n",
    "def normalize(train, test):\n",
    "    train_input_mean = train.signal.mean()\n",
    "    train_input_sigma = train.signal.std()\n",
    "    train['signal'] = (train.signal - train_input_mean) / train_input_sigma\n",
    "    test['signal'] = (test.signal - train_input_mean) / train_input_sigma\n",
    "    return train, test\n",
    "\n",
    "\n",
    "def lag_with_pct_change(df, windows):\n",
    "    for window in windows:    \n",
    "        df['signal_shift_pos_' + str(window)] = df.groupby('group')['signal'].shift(window).fillna(0)\n",
    "        df['signal_shift_neg_' + str(window)] = df.groupby('group')['signal'].shift(-1 * window).fillna(0)\n",
    "    return df\n",
    "\n",
    "\n",
    "def run_feat_engineering(df, batch_size):\n",
    "    df = batching(df, batch_size=batch_size)\n",
    "    df = lag_with_pct_change(df, [1, 2, 3])\n",
    "    df['signal_2'] = df['signal'] ** 2\n",
    "    return df\n",
    "\n",
    "\n",
    "def feature_selection(train, test):\n",
    "    features = [col for col in train.columns if col not in ['index', 'group', 'open_channels', 'time']]\n",
    "    train = train.replace([np.inf, -np.inf], np.nan)\n",
    "    test = test.replace([np.inf, -np.inf], np.nan)\n",
    "    for feature in features:\n",
    "        feature_mean = pd.concat([train[feature], test[feature]], axis=0).mean()\n",
    "        train[feature] = train[feature].fillna(feature_mean)\n",
    "        test[feature] = test[feature].fillna(feature_mean)\n",
    "    return train, test, features\n",
    "\n",
    "# stacking flipped X's\n",
    "def augment(X, y): \n",
    "    X = np.vstack((X, np.flip(X, axis=1), np.flip(X, axis=0)))\n",
    "    y = np.vstack((y, np.flip(y, axis=1), np.flip(y, axis=0)))\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mish(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Mish, self).__init__(**kwargs)\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return inputs * K.tanh(K.softplus(inputs))\n",
    "\n",
    "    def get_config(self):\n",
    "        base_config = super(Mish, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "def mish(x):\n",
    "\treturn tf.keras.layers.Lambda(lambda x: x*K.tanh(K.softplus(x)))(x)\n",
    " \n",
    "get_custom_objects().update({'mish': Activation(mish)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Classifier(shape_):\n",
    "    \n",
    "    def cbm(x, out_layer, kernel, stride, dilation):\n",
    "        x = Conv1D(out_layer, \n",
    "                   kernel_size=kernel, \n",
    "                   dilation_rate=dilation, \n",
    "                   strides=stride, \n",
    "                   padding=\"same\")(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation(\"mish\")(x)\n",
    "        return x\n",
    "    \n",
    "    def wave_block(x, filters, kernel_size, n):\n",
    "        dilation_rates = [2**i for i in range(n)]\n",
    "        x = Conv1D(filters=filters,\n",
    "                   kernel_size=1,\n",
    "                   padding='same')(x)\n",
    "        res_x = x\n",
    "        for dilation_rate in dilation_rates:\n",
    "            tanh_out = Conv1D(filters=filters,\n",
    "                              kernel_size=kernel_size,\n",
    "                              padding='same', \n",
    "                              activation='tanh', \n",
    "                              dilation_rate=dilation_rate)(x)\n",
    "            sigm_out = Conv1D(filters=filters,\n",
    "                              kernel_size=kernel_size,\n",
    "                              padding='same',\n",
    "                              activation='sigmoid', \n",
    "                              dilation_rate=dilation_rate)(x)\n",
    "            x = Multiply()([tanh_out, sigm_out])\n",
    "            x = Conv1D(filters=filters,\n",
    "                       kernel_size=1,\n",
    "                       padding='same')(x)\n",
    "            res_x = Add()([res_x, x])\n",
    "        return res_x\n",
    "    \n",
    "    inp = Input(shape=(shape_))\n",
    "    x = cbm(inp, 64, 7, 1, 1)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = wave_block(x, 16, 3, 12)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = wave_block(x, 32, 3, 8)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = wave_block(x, 64, 3, 4)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = wave_block(x, 128, 3, 1)\n",
    "    x = cbm(x, 32, 7, 1, 1)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    out = Dense(11, activation='softmax', name='out')(x)\n",
    "    model = models.Model(inputs=inp, outputs=out)\n",
    "    radam = tfa.optimizers.RectifiedAdam()\n",
    "    opt = tfa.optimizers.Lookahead(radam)\n",
    "    model.compile(loss=losses.CategoricalCrossentropy(), \n",
    "                  optimizer=opt, \n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MacroF1(Callback):\n",
    "    \n",
    "    def __init__(self, model, inputs, targets):\n",
    "        self.model = model\n",
    "        self.inputs = inputs\n",
    "        self.targets = np.argmax(targets, axis=2).reshape(-1)\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        pred = np.argmax(self.model.predict(self.inputs), axis=2).reshape(-1)\n",
    "        score = f1_score(self.targets, pred, average='macro')\n",
    "        print(f'F1 Macro Score: {score:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 26655557,
     "status": "ok",
     "timestamp": 1587071874123,
     "user": {
      "displayName": "佐藤幸志朗",
      "photoUrl": "",
      "userId": "03692871971235645955"
     },
     "user_tz": -540
    },
    "id": "DVQfDwKlb7f0",
    "outputId": "9a75e5e8-7570-461c-a8da-11d30150cf3d"
   },
   "outputs": [],
   "source": [
    "def run_cv_model_by_batch(train, test, splits, batch_col, feats, sample_submission, nn_epochs, nn_batch_size):\n",
    "    # set configuration\n",
    "    seed_everything(SEED)\n",
    "    K.clear_session()\n",
    "    config = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "    sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=config)\n",
    "    tf.compat.v1.keras.backend.set_session(sess)\n",
    "    # preparation for groupkfold\n",
    "    oof_ = np.zeros((len(train), 11)) \n",
    "    preds_ = np.zeros((len(test), 11))\n",
    "    target = ['open_channels']\n",
    "    group = train['group']\n",
    "    kf = GroupKFold(n_splits=5)\n",
    "    splits = [x for x in kf.split(train, train[target], group)]\n",
    "    new_splits = []\n",
    "    for sp in splits:\n",
    "        new_split = []\n",
    "        new_split.append(np.unique(group[sp[0]]))\n",
    "        new_split.append(np.unique(group[sp[1]]))\n",
    "        new_split.append(sp[1])    \n",
    "        new_splits.append(new_split)\n",
    "    \n",
    "    tr = pd.concat([pd.get_dummies(train.open_channels), train[['group']]], axis=1)\n",
    "\n",
    "    tr.columns = ['target_'+str(i) for i in range(11)] + ['group']\n",
    "    target_cols = ['target_'+str(i) for i in range(11)]\n",
    "    train_tr = np.array(list(tr.groupby('group').apply(lambda x: x[target_cols].values))).astype(np.float32)\n",
    "    train = np.array(list(train.groupby('group').apply(lambda x: x[feats].values)))\n",
    "    test = np.array(list(test.groupby('group').apply(lambda x: x[feats].values)))\n",
    "    # training and predict loop\n",
    "    for n_fold, (tr_idx, val_idx, val_orig_idx) in enumerate(new_splits[0:], start=0):\n",
    "        train_x, train_y = train[tr_idx], train_tr[tr_idx]\n",
    "        valid_x, valid_y = train[val_idx], train_tr[val_idx]\n",
    "        train_x, train_y = augment(train_x, train_y)\n",
    "\n",
    "        gc.collect()\n",
    "        shape_ = (None, train_x.shape[2]) \n",
    "        model = Classifier(shape_)\n",
    "        checkpoint = ModelCheckpoint('best_model_weight_' + str(n_fold+1) +'.h5', verbose=1, \n",
    "                                     save_best_only=True, save_weights_only=True)\n",
    "        model.fit(train_x, train_y,\n",
    "                  epochs=nn_epochs,\n",
    "                  callbacks=[checkpoint, MacroF1(model, valid_x, valid_y)], \n",
    "                  batch_size=nn_batch_size, verbose=1,\n",
    "                  validation_data=(valid_x, valid_y))\n",
    "        model.load_weights('best_model_weight_' + str(n_fold+1) +'.h5')\n",
    "        \n",
    "        preds_f = model.predict(valid_x)\n",
    "        f1_score_ = f1_score(np.argmax(valid_y, axis=2).reshape(-1),  np.argmax(preds_f, axis=2).reshape(-1), average='macro') \n",
    "        print(f'Training fold {n_fold + 1} completed. macro f1 score : {f1_score_ :1.5f}')\n",
    "        preds_f = preds_f.reshape(-1, preds_f.shape[-1])\n",
    "        oof_[val_orig_idx, :] += preds_f\n",
    "        te_preds = model.predict(test)\n",
    "        te_preds = te_preds.reshape(-1, te_preds.shape[-1])           \n",
    "        preds_ += te_preds / SPLITS\n",
    "    \n",
    "    f1_score_ = f1_score(np.argmax(train_tr, axis=2).reshape(-1),  np.argmax(oof_, axis=1), average='macro')\n",
    "    print(f'Training completed. oof macro f1 score : {f1_score_:1.5f}')\n",
    "    sample_submission['open_channels'] = np.argmax(preds_, axis=1).astype(int)\n",
    "    sample_submission.to_csv('submission.csv', index=False, float_format='%.4f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_everything():\n",
    "    \n",
    "    print('Reading Data Started...')\n",
    "    train, test, sample_submission = read_data()\n",
    "    train, test = normalize(train, test)\n",
    "    print('Reading and Normalizing Data Completed...')\n",
    "        \n",
    "    print('Creating Features')\n",
    "    print('Feature Engineering Started...')\n",
    "    train = run_feat_engineering(train, batch_size=GROUP_BATCH_SIZE)\n",
    "    test = run_feat_engineering(test, batch_size=GROUP_BATCH_SIZE)\n",
    "    train, test, features = feature_selection(train, test)\n",
    "    print('Feature Engineering Completed...')\n",
    "        \n",
    "   \n",
    "    print(f'Training Wavenet model with {SPLITS} folds of GroupKFold Started...')\n",
    "    run_cv_model_by_batch(train, test, SPLITS, 'group', features, sample_submission, EPOCHS, NNBATCHSIZE)\n",
    "    print('Training completed')\n",
    "        \n",
    "run_everything()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "WaveNet-Keras_Radam+SHIFTED-RFC Proba.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
